{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2558bebd-89da-4945-ac50-ef686b87ccd2",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "   \n",
    "2. [Data Preparation](#Data-Preparation)\n",
    "\n",
    "   2.1 [ST prepration](##ST-prepration)\n",
    "   \n",
    "   2.2 [WES prepration](##WES-prepration)\n",
    "   \n",
    "3. [Tumoroscope](#Tumoroscope)\n",
    "\n",
    "   3.1 [OS Requirements](#OS-Requirements)\n",
    "   \n",
    "   3.2 [Python Dependencies](#Python-Dependencies)\n",
    "   \n",
    "   3.3 [Install from PyPi](#Install-from-PyPi)\n",
    "   \n",
    "   3.4 [Example of using Tumoroscope](#Example-of-using-Tumoroscope)\n",
    "   \n",
    "4. [Validation](#Validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f76b6-5904-4410-9139-6ae3a62e4901",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc7127-682e-4e5d-9edc-a50b5732750c",
   "metadata": {},
   "source": [
    "In this vignette, we provide a hands-on walkthrough of using the Tumoroscope model to analyze cancer sequencing data. We start with an overview of loading and processing raw sequencing data. We then cover the key parameters and usage of the Tumoroscope model for clone mapping. Finally, we demonstrate tools for visualizing and interpreting the subclonal populations predicted by Tumoroscope.\n",
    "\n",
    "This vignette aims to make the Tumoroscope workflow accessible to cancer researchers interested in studying clonal dynamics. Working through the examples will build intuition for how Tumoroscope integrates spatial transcriptomics, bulk sequencing data and H&E images into a high-resolution map of intratumor heterogeneity. The provided code serves as a starting point for researchers to apply Tumoroscope to their own cancer genomics datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6f478-c362-4f51-81c5-cf8778b357e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data-Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0728f53-8cee-4159-a88e-95978c485e46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ST-prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae885dd4-d9e3-4f94-a0b3-113731cca3a6",
   "metadata": {},
   "source": [
    "We utilized the st_pipeline available at https://github.com/jfnavarro/st_pipeline to process our raw Spatial Transcriptomics sequencing data. We recommend users apply the st_pipeline to their ST FASTQ files before proceeding with further analysis. For this study, we used the BAM files that are an interim output, but are needed specifically for mutation calling before running Tumoroscope. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564685a0-0b4f-4946-8948-7896d89980e8",
   "metadata": {},
   "source": [
    "The ST pipeline begins by building a genome index using STAR. This index will be used to align the sequencing reads to the reference genome. We specify parameters like the path to the reference FASTA genome file, the GTF annotation file, and the overhang length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f45af9-33ed-4199-a507-b5b7e2017a2b",
   "metadata": {},
   "source": [
    "```\n",
    "STAR --runThreadN 10 \\\n",
    "--runMode genomeGenerate \\\n",
    "--genomeDir output \\\n",
    "--genomeFastaFiles /home/shafighi/star/fasta/GRCh38.primary_assembly.genome.fa \\\n",
    "--sjdbGTFfile /home/shafighi/star/gtcf/gencode.v35.annotation.gtf \\\n",
    "--sjdbOverhang 99\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa63a46-c2c9-42d2-8794-496d1e857c5a",
   "metadata": {},
   "source": [
    "The basic command for running the pipeline is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e63ea7-230e-46c2-b9ff-76456653b0c5",
   "metadata": {},
   "source": [
    "```\n",
    "st_pipeline_run.py --expName test \\\n",
    "                --ids /home/shafighi/alireza/stpipeline/index/1000L9_barcodes.txt \\\n",
    "                --ref-map /home/shafighi/alireza/stpipeline/star/ \n",
    "                --log-file log_file.txt \\\n",
    "                --output-folder /home/shafighi/alireza/stpipeline/output /home/shafighi/alireza/stpipeline/fasta/EGAR00001746265_Patient1_2.4_R1.fastq /home/shafighi/alireza/stpipeline/fasta/EGAR00001746265_Patient1_2.4_R2.fastq \n",
    "                --transcriptome --no-clean-up --temp-folder /home/shafighi/alireza/stpipeline/tmp\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a899f-904d-4a64-a236-a05a07d1ce63",
   "metadata": {},
   "source": [
    "See the st_pipeline GitHub repository for details on all the available parameters and customization options. Specifically, you need ST barcodes and the directory of the STAR results. We used \"--no-clean-up\" parameter to have the BAM files as outputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f48c9-39f8-4ba2-a12a-749399e1fbba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Calling mutations from ST BAM files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf64c2-ecb9-4f62-83d8-879782759ae4",
   "metadata": {},
   "source": [
    "Since standard mutation callers cannot be directly applied to Spatial Transcriptomics BAM files, we developed a custom tool called read_counter (https://github.com/shafighi/read_counter) to identify mutation alleles for input to Tumoroscope.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c010d-1f2d-400a-a03e-ed1452b9659c",
   "metadata": {},
   "source": [
    "```python st_read_counter.py st_bam_file  st_bai_file  barcode  vcf_file  output_name  tumor_column_vcf st_dir```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f66e003-842e-4d18-bbcf-c27b078550ff",
   "metadata": {},
   "source": [
    "The `st_read_counter.py` script is designed to streamline the process of analyzing Spatial Transcriptomics (ST) data to get the allelic read counts. It requires several input files and parameters to operate effectively:\n",
    "\n",
    "1. `st_bam_file` and `st_bai_file`: These are the BAM and BAI files containing the ST data.\n",
    "\n",
    "2. `barcode`: This parameter represents the barcode or identifier associated with the st sample, which is crucial for identifying and distinguishing the position of the data points.\n",
    "\n",
    "3. `vcf_file`: The VCF (Variant Call Format) file contains information about selected mutations from Whole Exome Sequencing (WES). \n",
    "\n",
    "4. `output_name`: The name of the output file.\n",
    "\n",
    "5. `tumor_column_vcf`: This specifies the column in the VCF file that corresponds to the sample.\n",
    "\n",
    "6. `st_dir`: This parameter defines the directory where the program will generate the necessary output files.\n",
    "\n",
    "The script's primary function is to process the provided ST BAM files, integrate them with the relevant VCF data, and generate allele counts for each potential variant site. These allele counts serve as essential inputs for downstream analysis tools like Tumoroscope, enabling the inference of subclonal populations across the spatial spots.\n",
    "\n",
    "To gain comprehensive usage details and further insights into the functionality of `st_read_counter.py`, please refer to the `read_counter` repository and associated documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e38b3a-c094-4a91-bcbb-898043c458ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## WES-prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d609408-0f13-4039-815d-8dda9b7393d1",
   "metadata": {},
   "source": [
    "### Applying FalconX and Canopy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50473180-a859-4933-95e9-7195698e67f2",
   "metadata": {},
   "source": [
    "We utilized Canopy (https://github.com/yuchaojiang/Canopy) to infer subclones from the mutation calls. Canopy takes the mutation alleles files along with copy number segments as input. To obtain copy number, we used FalconX as described in the Canopy repository (https://github.com/yuchaojiang/Canopy/blob/master/instruction/SNA_CNA_input.md). FalconX segments the genome based on read depth and outputs copy number calls for each segment. With the mutation VCFs and copy number segments, Canopy constructs a clonal tree using a Bayesian nonparametric mixture model. It outputs the tree structure, genotype, and frequency of the clones. Consult the Canopy documentation for full usage details. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd59536a-3a2d-41e6-ab57-56e6358eca79",
   "metadata": {},
   "source": [
    "# Tumoroscope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dda446-29fd-454e-a070-609d0d91069f",
   "metadata": {},
   "source": [
    "### OS Requirements\n",
    "The package has been tested on the following systems:\n",
    "+ macOS (ventura 13.0 and Catalina 10.15.7)\n",
    "+ Linux (ubuntu 14.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b16dc-cd9a-4636-b3ab-2f012869aee2",
   "metadata": {},
   "source": [
    "### Python Dependencies\n",
    "`tumoroscope` mainly depends on the Python scientific stack.\n",
    "\n",
    "```\n",
    "numpy\n",
    "pandas\n",
    "matplotlib\n",
    "seaborn\n",
    "pickle5\n",
    "scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a5663b-a9c2-4ee4-86a3-f8a3d59ea29f",
   "metadata": {},
   "source": [
    "### Install from PyPi\n",
    "Once you've successfully installed all the necessary libraries, installing Tumoroscope is a straightforward process. Simply execute the following command:\n",
    "\n",
    "```\n",
    "pip3 install tumoroscope\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf3500-02d5-412c-b444-6506efbda962",
   "metadata": {},
   "source": [
    "### Example of using Tumoroscope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa09c8c-1cb9-404f-b0eb-603e8329f884",
   "metadata": {},
   "source": [
    "To get started, it's essential to import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae451ba-eb5c-4dbb-87f0-8017b0cd514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tumoroscope.run_tumoroscope import run_tumoroscope\n",
    "from tumoroscope.simulation import simulation as sim\n",
    "from tumoroscope.visualization import visualization as vis\n",
    "from tumoroscope.tumoroscope import tumoroscope as tum\n",
    "from tumoroscope import constants\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb216d-33d8-4531-9e96-2db4478db7ee",
   "metadata": {},
   "source": [
    "Next, you'll proceed to parameter initialization. It's crucial to configure key settings such as the \"config_file,\" visualization preferences, and the output folder. Additionally, you'll need to specify the available number of cores and determine the desired number of chains for running Tumoroscope efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf29c17-f937-4c0d-ab3a-6f47725951ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 471 µs, sys: 312 µs, total: 783 µs\n",
      "Wall time: 687 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pi_2D = True\n",
    "th = 0.8 # threshhold for Z\n",
    "constants.VISUALIZATION = 'visualization'\n",
    "constants.CHAINS = 1\n",
    "constants.CORES = 35\n",
    "every_n_sample = 5\n",
    "l=1\n",
    "changes_batch= 500\n",
    "constants.RESULTS = 'Result_demo'\n",
    "number = '1'\n",
    "config_file =  '../configs/simulation_demo.json'\n",
    "result_txt = constants.RESULTS+'/results_'\n",
    "optimal_rate = 0.40\n",
    "test = constants.RESULTS\n",
    "begin= time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e7930-bf1b-4f10-8f1b-5eeb83f9e120",
   "metadata": {},
   "source": [
    "Then, we create the output directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102b1841-a3ef-4a9e-b737-3530be31ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  Result_demo  already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(constants.RESULTS):\n",
    "    os.makedirs(constants.RESULTS)\n",
    "    print(\"Directory \", constants.RESULTS, \" Created \")\n",
    "else:\n",
    "    print(\"Directory \", constants.RESULTS, \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab260d4c-8870-4bf8-9e7c-5fbdd664e726",
   "metadata": {},
   "source": [
    "Afterwards, we read the config file to set the parameters of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc14701-4c17-4c13-a64b-d40158cdd364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  Result_demo/n_var_simulation_demo_visualization  already exists\n"
     ]
    }
   ],
   "source": [
    "file_name = re.split(\"/|.json\",config_file)[2]\n",
    "vis_1 = vis(constants.RESULTS + '/n_var_' + file_name + '_' + constants.VISUALIZATION)\n",
    "with open(config_file) as json_data_file:\n",
    "    data = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb316b-bd9b-4c1f-a60b-24e04f888ceb",
   "metadata": {},
   "source": [
    "Here are the parameters that we need to specify in the config file. K, S and I are specifying the number of clones, number of spots and the number of mutations that we are using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0074ef3d-af92-461d-8599-dee41e210c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = data['structure']['K']\n",
    "S = data['structure']['S']\n",
    "I = data['structure']['I']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83a95a-e62b-4397-b14f-669b8d5c30a0",
   "metadata": {},
   "source": [
    "Please set theta to 1 and p_c_binom to null. These are regarding to some features that we are not supporting at the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cacebe80-6f7e-4d5f-934a-97142452f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = data['structure']['theta']\n",
    "p_c_binom = data['C_variation']['p_c_binom']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20e95a-2ab6-4ff7-b92c-c882ce8afccc",
   "metadata": {},
   "source": [
    "In the next part we are setting the  genotype by setting different pattern to \"C\" and the number of repetition of these patterns to \"repeat\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "597459e8-9f5b-4d22-8d42-ef6e9b641487",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_temp = data['C_variation']['C']\n",
    "repeat_temp = data['C_variation']['repeat']\n",
    "if C_temp is None:\n",
    "    C = None\n",
    "else:\n",
    "    C_t = []\n",
    "    for ct in range(len(C_temp)):\n",
    "        C_t.append(np.tile(C_temp[ct], (repeat_temp[ct], 1)))\n",
    "    C = np.concatenate(C_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6835b3-d773-4976-a7cb-62b43bc4c382",
   "metadata": {},
   "source": [
    "In the n_variation section, we should specify if we are sampling n or not by setting n_sampling to true or false. n_lamda should be set to the expected number of cells in a spot and n should be set to the observed number of cells in the spots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62cfb2eb-0634-465f-aa7c-101defd32b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sampling = data['n_variation']['n_sampling']\n",
    "n_lambda = np.tile(data['n_variation']['n_lambda'], (S))\n",
    "\n",
    "if data['n_variation']['n'] is not None:\n",
    "    n = np.array(data['n_variation']['n'])\n",
    "else:\n",
    "    n = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e43d60-c9cb-4f80-a093-a98a90235f13",
   "metadata": {},
   "source": [
    "It's essential to define the \"average_clone_in_spot\" parameter, which represents your anticipated average number of clones within each spot. This estimation should be grounded in your understanding of the sample characteristics and the level of heterogeneity you anticipate. Please set Z to null. This parameter is not supported yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27600336-ce27-4a72-9e2b-7e76dc215d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = data['Z_variation']['Z']\n",
    "avarage_clone_in_spot = data['Z_variation']['avarage_clone_in_spot']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccdcb8e-d9ba-40b8-a69b-7708023808d4",
   "metadata": {},
   "source": [
    "Then we need to set the parameters regarding the sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b440a633-4ce4-41db-abc7-224edeb86024",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = int(data['smapling']['max_iter'])\n",
    "min_iter = int(data['smapling']['min_iter'])\n",
    "burn_in = int(data['smapling']['burn_in'])\n",
    "batch = int(data['smapling']['batch'])\n",
    "var_calculation = int(data['smapling']['min_iter']*0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f26e2-5db1-4ea2-a0ce-09bb5b26267f",
   "metadata": {},
   "source": [
    "Within this segment, you have the flexibility to define variables relating to read counts and clone frequencies. It's noteworthy that parameters lacking the \"_selected\" suffix pertain to the actual values, whereas those with the \"_selected\" suffix correspond to user-provided estimates. This distinction arises from the fact that, in actual datasets, true values are often elusive, necessitating user approximations. As such, distinct parameters cater to both real data and user-inferred estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43ae0899-21d4-49a5-b7a0-bdab38bb0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_gamma = np.array(data['Gamma']['phi_gamma'])\n",
    "        # F could be None, In that case, it will be generated using dirichlet distribution\n",
    "F_epsilon = np.tile(data['Gamma']['F_epsilon'], (K, 1))\n",
    "F_fraction =  data['Gamma']['F_fraction']\n",
    "\n",
    "F = np.tile(data['Gamma']['F'], (K, 1))  # np.array([[9,2],[9,2],[9,2],[9,2],[9,2],[9,2]])\n",
    "phi_gamma_selected = np.array(data['Gamma']['phi_gamma_selected'])\n",
    "\n",
    "F_epsilon_selected = np.tile(data['Gamma']['F_epsilon_selected'], (K, 1))\n",
    "F_selected = np.tile(data['Gamma']['F_selected'], (K, 1))\n",
    "\n",
    "  # np.array([[9,2],[9,2],[9,2],[9,2],[9,2],[9,2]])\n",
    "gamma = data['theta']['gamma']\n",
    "gamma_sampling = data['theta']['gamma_sampling']\n",
    "theta_variable = data['theta']['theta_variable']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dcfb6-1dfb-4f36-ad69-47035ba2b43b",
   "metadata": {},
   "source": [
    "In the next step, we generate the simulated data using sim function that we imported before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95c3e2a-9ef1-4495-aff3-1c9f353c939e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation finished\n"
     ]
    }
   ],
   "source": [
    "sample_1 = sim(K=K,S=S,r=phi_gamma[0],p=phi_gamma[1],I=I,F=F,D=None,A=None,C=C,avarage_clone_in_spot=avarage_clone_in_spot,random_seed=random.randint(1,100),F_epsilon= F_epsilon,n=n,p_c_binom=p_c_binom,theta=theta,Z = Z,n_lambda=n_lambda,F_fraction=F_fraction,theta_variable=theta_variable,gamma=gamma, pi_2D=pi_2D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0c5e7-6a1e-4a7e-85e4-c71097d2e34d",
   "metadata": {},
   "source": [
    "At this stage, we introduce noise into the dataset, with the noise intensity contingent upon the mean value of the Poisson distribution. This mean value is modulated by user-defined noise parameters, affording precise control over the noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3360e3f6-ec63-4ea4-a944-d7c8b16c769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = '1'\n",
    "if noise=='0':\n",
    "    n_lambda_tum = sample_1.n\n",
    "else:\n",
    "    b = np.random.binomial(n=1,p=0.5,size=S)\n",
    "    noise_pois = np.random.poisson(lam=int(noise),size=S)\n",
    "    n_lambda_tum = sample_1.n+noise_pois*b+noise_pois*(b-1)\n",
    "    n_lambda_tum[n_lambda_tum<1]=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251d26b7-d3ec-402d-a84d-de1c9b1ad2df",
   "metadata": {},
   "source": [
    "In the next step, we use Tumoroscope to infer the clones in the spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ee3c6-be0e-43f0-b2dc-00e33eac0407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation finished\n",
      "Batch 0 finished with [62.5  67.5  53.33 55.83 45.83 65.83]% convergence for [0,10,20,30,40,50]% burn-in.\n",
      "Batch 1 finished with [85.83 72.5  56.67 61.67 59.17 62.5 ]% convergence for [0,10,20,30,40,50]% burn-in.\n",
      "Batch 2 finished with [93.33 60.   60.   51.67 46.67 47.5 ]% convergence for [0,10,20,30,40,50]% burn-in.\n"
     ]
    }
   ],
   "source": [
    "tum_obj=(tum(name = constants.RESULTS +'/'+ file_name+'_chain_'+str(constants.CHAINS),K=sample_1.K,S=sample_1.S,r=phi_gamma_selected[0],p=phi_gamma_selected[1],I=sample_1.I,avarage_clone_in_spot=sample_1.avarage_clone_in_spot,F = F_selected,C = sample_1.C,A = sample_1.A,D = sample_1.D,F_epsilon=F_epsilon_selected,optimal_rate=optimal_rate,n_lambda = n_lambda_tum,gamma = gamma_sampling, pi_2D=pi_2D,result_txt=result_txt+file_name+'.txt'))\n",
    "tum_obj.gibbs_sampling(seed = random.randint(1,100) ,min_iter=min_iter,max_iter = max_iter,burn_in=burn_in, batch = batch,simulated_data= sample_1,n_sampling=True,F_fraction=F_fraction,theta_variable = theta_variable, pi_2D=pi_2D,th=th,every_n_sample=every_n_sample,changes_batch=changes_batch,var_calculation=var_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889bfcc5-c14e-4518-b8bb-410c70fc64c1",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1380d6-aeed-4259-b231-0bbfb937d746",
   "metadata": {},
   "source": [
    "We're creating a chart that compares the actual clones present in each spot with the clones we've estimated. This helps us confirm how well our estimates match the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5db077-4f03-44b7-b346-ac12e76eee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('classic')\n",
    "font_size=12\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.scatter(tum_obj.inferred_H.tolist(),sample_1.H.tolist())\n",
    "ax.set_ylabel('True fraction of the clones', fontsize=font_size,labelpad=0.05)\n",
    "ax.set_xlabel('Inferred fraction of the clones', fontsize=font_size,labelpad=0.05)\n",
    "ax.set_title('Mean Average Error (MAE): '+str(round(tum_obj.H_SEE,4)),fontsize=font_size, pad = 1.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
